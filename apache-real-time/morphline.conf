# Specify server locations in a SOLR_LOCATOR variable;
# used later in variable substitutions
# Change the zkHost to point to your own Zookeeper quorum
SOLR_LOCATOR : {
    # Name of solr collection
    collection : accessCollection
    # ZooKeeper ensemble
    zkHost : "localhost:2181/solr"
}

# Specify an array of one or more morphlines, each of which defines an ETL
# transformation chain. A morphline consists of one or more (potentially
# nested) commands. A morphline is a way to consume records (e.g. Flume events,
# HDFS files or blocks), turn them into a stream of records, and pipe the stream
# of records through a set of easily configurable transformations on it's way to
# Solr (or a MapReduceIndexerTool RecordWriter that feeds via a Reducer into Solr).
morphlines : [
{
    # Name used to identify a morphline. E.g. used if there are multiple morphlines in a
    # morphline config file
    id : morphline1
    # Import all morphline commands in these java packages and their subpackages.
    # Other commands that may be present on the classpath are not visible to this morphline.
    importCommands : ["org.kitesdk.**", "org.apache.solr.**"]
    commands : [
    {
        ## Read the email stream and break it up into individual messages.
        ## The beginning of a message is marked by regex clause below
        ## The reason we use this command is that one event can have multiple
        ## messages
        readCSV {
	    separator:  " "
            columns:  [client_ip,C1,C2,time,dummy1,request,code,bytes,referer,user_agent,C3]
	    ignoreFirstLine : false
            quoteChar : "\""
            commentPrefix : ""
            trim : true
            charset : UTF-8
        }
    }
    {
	split { 
		inputField : request
		outputFields : [method, url, protocol]          
		separator : " "        
		isRegex : false      
		#separator : """\s*,\s*"""        
		#  #isRegex : true      
		addEmptyStrings : false
		trim : true          
          }
    }
     {
	split { 
		inputField : url 
		outputFields : ["", app, subapp]          
		separator : "\/"        
		isRegex : false      
		#separator : """\s*,\s*"""        
		#  #isRegex : true      
		addEmptyStrings : false
		trim : true          
          }
    }
    {
	userAgent {
		inputField : user_agent
		outputFields : {
			user_agent_family : "@{ua_family}"
			user_agent_major  : "@{ua_major}"
			device_family     : "@{device_family}"
			os_family         : "@{os_family}"
			os_major	  : "@{os_major}"
		}          
	}
    }
    {
	#Extract GEO information
	geoIP {
            inputField : client_ip
            database : "/tmp/GeoLite2-City.mmdb"
	}
     }
     {

	# extract parts of the geolocation info from the Jackson JsonNode Java 
	# # object contained in the _attachment_body field and store the parts in
	# # the given record output fields:      
	extractJsonPaths {
	flatten : false
	paths : { 
		country_code : /country/iso_code
		country_name : /country/names/en
                region_code  : /continent/code
		#"/subdivisions[]/names/en" : "/subdivisions[]/names/en"     
		#"/subdivisions[]/iso_code" : "/subdivisions[]/iso_code"     
		city : /city/names/en
		#/postal/code : /postal/code
		latitude : /location/latitude
		longitude : /location/longitude
		#/location/latitude_longitude : /location/latitude_longitude
		#/location/longitude_latitude : /location/longitude_latitude
		} 
	}
      }
      #{logInfo { format : "BODY : {}", args : ["@{}"] } }
    # add Unique ID, in case our message_id field from above is not present
    {
        generateUUID {
            field:id
        }
    }

    # convert the timestamp field to "yyyy-MM-dd'T'HH:mm:ss.SSSZ" format
    {
       #  21/Nov/2014:22:08:27
        convertTimestamp {
            field : time 
            inputFormats : ["[dd/MMM/yyyy:HH:mm:ss", "EEE, d MMM yyyy HH:mm:ss Z", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'", "yyyy-MM-dd'T'HH:mm:ss", "yyyy-MM-dd"]
            inputTimezone : America/Los_Angeles
           outputFormat : "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"
            outputTimezone : UTC
        }
    }

    # Consume the output record of the previous command and pipe another
    # record downstream.
    #
    # This command sanitizes record fields that are unknown to Solr schema.xml
    # by deleting them. Recall that Solr throws an exception on any attempt to
    # load a document that contains a field that isn't specified in schema.xml
    {
        sanitizeUnknownSolrFields {
            # Location from which to fetch Solr schema
            solrLocator : ${SOLR_LOCATOR}
        }
    }

    # load the record into a SolrServer or MapReduce SolrOutputFormat.
    {
        loadSolr {
            solrLocator : ${SOLR_LOCATOR}
        }
    }
    ]
}
]


